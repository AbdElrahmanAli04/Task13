{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, sizes):\n",
        "        # `sizes` is a list containing the number of neurons in each layer (input, hidden, output)\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        # Initialize weights and biases with random values from a normal distribution\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        # Sigmoid activation function\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        # Derivative of the sigmoid function\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        # Pass input `a` through the network\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = self.sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def backpropagation(self, x, y):\n",
        "        # Backpropagation algorithm to compute the gradient of the loss function\n",
        "        # Initialize gradient arrays\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]  # List to store all activations, layer by layer\n",
        "        zs = []  # List to store all z vectors, layer by layer (z = w*x + b)\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        # Compute the output error (delta)\n",
        "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_derivative(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        # Backpropagate the error\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.sigmoid_derivative(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        # Update the network's weights and biases by applying gradient descent using backpropagation\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def train(self, training_data, epochs, mini_batch_size, eta):\n",
        "        # Train the neural network using mini-batch stochastic gradient descent\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, len(training_data), mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            print(f\"Epoch {epoch} complete\")\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        # Return the derivative of the cost function\n",
        "        return (output_activations - y)\n",
        "\n",
        "# Example usage:\n",
        "# Create a neural network with 784 input neurons, 30 hidden neurons, and 10 output neurons (MNIST dataset format)\n",
        "nn = NeuralNetwork([784, 30, 10])\n",
        "\n",
        "# Example random training data (In practice, use actual MNIST data)\n",
        "# Each `x` is a 784x1 vector (flattened 28x28 image), each `y` is a 10x1 vector (one-hot encoded digit label)\n",
        "training_data = [(np.random.randn(784, 1), np.random.randn(10, 1)) for _ in range(1000)]\n",
        "\n",
        "# Train the neural network for 30 epochs with a mini-batch size of 10 and a learning rate of 3.0\n",
        "nn.train(training_data, epochs=30, mini_batch_size=10, eta=3.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clnS7wZiaV3P",
        "outputId": "bb7ad827-7020-453d-ba56-6c59792e4909"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 complete\n",
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Epoch 4 complete\n",
            "Epoch 5 complete\n",
            "Epoch 6 complete\n",
            "Epoch 7 complete\n",
            "Epoch 8 complete\n",
            "Epoch 9 complete\n",
            "Epoch 10 complete\n",
            "Epoch 11 complete\n",
            "Epoch 12 complete\n",
            "Epoch 13 complete\n",
            "Epoch 14 complete\n",
            "Epoch 15 complete\n",
            "Epoch 16 complete\n",
            "Epoch 17 complete\n",
            "Epoch 18 complete\n",
            "Epoch 19 complete\n",
            "Epoch 20 complete\n",
            "Epoch 21 complete\n",
            "Epoch 22 complete\n",
            "Epoch 23 complete\n",
            "Epoch 24 complete\n",
            "Epoch 25 complete\n",
            "Epoch 26 complete\n",
            "Epoch 27 complete\n",
            "Epoch 28 complete\n",
            "Epoch 29 complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to load and preprocess MNIST dataset\n",
        "def load_mnist_data():\n",
        "    # Load the MNIST dataset using tensorflow.keras.datasets\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize the images (convert from 0-255 to 0-1)\n",
        "    train_images = train_images / 255.0\n",
        "    test_images = test_images / 255.0\n",
        "\n",
        "    # Flatten the 28x28 images into vectors of size 784\n",
        "    train_images = [image.reshape(784, 1) for image in train_images]\n",
        "    test_images = [image.reshape(784, 1) for image in test_images]\n",
        "\n",
        "    # Convert labels to one-hot encoded vectors\n",
        "    train_labels = [one_hot_encode(label) for label in train_labels]\n",
        "    test_labels = [one_hot_encode(label) for label in test_labels]\n",
        "\n",
        "    # Combine images and labels for training and testing data\n",
        "    training_data = list(zip(train_images, train_labels))\n",
        "    test_data = list(zip(test_images, test_labels))\n",
        "\n",
        "    return training_data, test_data\n",
        "\n",
        "def one_hot_encode(label):\n",
        "    # Create a 10x1 vector with all zeros, except a 1 at the index of the label\n",
        "    one_hot = np.zeros((10, 1))\n",
        "    one_hot[label] = 1.0\n",
        "    return one_hot\n",
        "\n",
        "# Now let's load the data\n",
        "training_data, test_data = load_mnist_data()\n",
        "\n",
        "# You can now train your neural network using this data:\n",
        "# Example of creating and training the neural network\n",
        "nn = NeuralNetwork([784, 40, 10])  # 784 input, 30 hidden, 10 output neurons\n",
        "nn.train(training_data, epochs=40, mini_batch_size=10, eta=3.0)\n",
        "\n",
        "# Optionally, write a method to test the accuracy on the test data\n",
        "def evaluate(network, test_data):\n",
        "    test_results = [(np.argmax(network.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
        "    return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "# Evaluate the network performance\n",
        "accuracy = evaluate(nn, test_data)\n",
        "print(f\"Test Accuracy: {accuracy} / {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZttRVjvWaqmY",
        "outputId": "c2227eb2-a1a2-41bb-cac9-6286e6949417"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 complete\n",
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Epoch 4 complete\n",
            "Epoch 5 complete\n",
            "Epoch 6 complete\n",
            "Epoch 7 complete\n",
            "Epoch 8 complete\n",
            "Epoch 9 complete\n",
            "Epoch 10 complete\n",
            "Epoch 11 complete\n",
            "Epoch 12 complete\n",
            "Epoch 13 complete\n",
            "Epoch 14 complete\n",
            "Epoch 15 complete\n",
            "Epoch 16 complete\n",
            "Epoch 17 complete\n",
            "Epoch 18 complete\n",
            "Epoch 19 complete\n",
            "Epoch 20 complete\n",
            "Epoch 21 complete\n",
            "Epoch 22 complete\n",
            "Epoch 23 complete\n",
            "Epoch 24 complete\n",
            "Epoch 25 complete\n",
            "Epoch 26 complete\n",
            "Epoch 27 complete\n",
            "Epoch 28 complete\n",
            "Epoch 29 complete\n",
            "Epoch 30 complete\n",
            "Epoch 31 complete\n",
            "Epoch 32 complete\n",
            "Epoch 33 complete\n",
            "Epoch 34 complete\n",
            "Epoch 35 complete\n",
            "Epoch 36 complete\n",
            "Epoch 37 complete\n",
            "Epoch 38 complete\n",
            "Epoch 39 complete\n",
            "Test Accuracy: 9557 / 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVCLGh0fjpCv",
        "outputId": "a6c9d025-1758-43cf-acac-dbb2543dc7c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 784    # 28x28 images\n",
        "hidden_size = 40    # 40 neurons in the hidden layer\n",
        "output_size = 10    # 10 classes for digits 0-9\n",
        "num_epochs = 40     # Number of times the training data is used\n",
        "batch_size = 64     # Batch size for training\n",
        "learning_rate = 0.003  # Learning rate\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Neural Network Model using PyTorch\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer (784 -> 30)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Fully connected layer (30 -> 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, input_size)  # Flatten the images into 784x1\n",
        "        x = torch.sigmoid(self.fc1(x))  # Apply Sigmoid activation\n",
        "        x = torch.sigmoid(self.fc2(x))  # Apply Sigmoid activation\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()  # Cross entropy loss for multi-class classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # Stochastic gradient descent optimizer\n",
        "\n",
        "# Training the model\n",
        "def train_model():\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, labels in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "def test_model():\n",
        "    model.eval()  # Set the model to evaluation mode (disable dropout, batchnorm, etc.)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the 10,000 test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Train and test the model\n",
        "train_model()\n",
        "test_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQMJoJQajqS2",
        "outputId": "f0ccbb9f-d54f-4db8-929f-c1232d398478"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 18079446.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 493532.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3906583.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11839980.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/40], Loss: 2.2740\n",
            "Epoch [2/40], Loss: 2.2755\n",
            "Epoch [3/40], Loss: 2.2700\n",
            "Epoch [4/40], Loss: 2.2542\n",
            "Epoch [5/40], Loss: 2.2193\n",
            "Epoch [6/40], Loss: 2.2070\n",
            "Epoch [7/40], Loss: 2.2002\n",
            "Epoch [8/40], Loss: 2.1627\n",
            "Epoch [9/40], Loss: 2.1128\n",
            "Epoch [10/40], Loss: 2.1439\n",
            "Epoch [11/40], Loss: 2.1158\n",
            "Epoch [12/40], Loss: 2.0648\n",
            "Epoch [13/40], Loss: 2.0254\n",
            "Epoch [14/40], Loss: 2.0659\n",
            "Epoch [15/40], Loss: 2.0166\n",
            "Epoch [16/40], Loss: 2.0155\n",
            "Epoch [17/40], Loss: 1.9986\n",
            "Epoch [18/40], Loss: 1.9459\n",
            "Epoch [19/40], Loss: 1.9465\n",
            "Epoch [20/40], Loss: 1.9289\n",
            "Epoch [21/40], Loss: 1.9095\n",
            "Epoch [22/40], Loss: 1.9458\n",
            "Epoch [23/40], Loss: 1.8982\n",
            "Epoch [24/40], Loss: 1.9278\n",
            "Epoch [25/40], Loss: 1.8677\n",
            "Epoch [26/40], Loss: 1.8423\n",
            "Epoch [27/40], Loss: 1.8772\n",
            "Epoch [28/40], Loss: 1.8529\n",
            "Epoch [29/40], Loss: 1.8015\n",
            "Epoch [30/40], Loss: 1.8671\n",
            "Epoch [31/40], Loss: 1.7842\n",
            "Epoch [32/40], Loss: 1.7801\n",
            "Epoch [33/40], Loss: 1.8351\n",
            "Epoch [34/40], Loss: 1.7984\n",
            "Epoch [35/40], Loss: 1.7776\n",
            "Epoch [36/40], Loss: 1.7962\n",
            "Epoch [37/40], Loss: 1.8171\n",
            "Epoch [38/40], Loss: 1.8087\n",
            "Epoch [39/40], Loss: 1.8131\n",
            "Epoch [40/40], Loss: 1.7773\n",
            "Accuracy of the model on the 10,000 test images: 85.02%\n"
          ]
        }
      ]
    }
  ]
}